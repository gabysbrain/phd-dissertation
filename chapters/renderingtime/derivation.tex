\section{Derivation of scene geometry}
\label{sec:derivation}

We now turn to a formulation for the expected total running time to draw $N$
points in $d$ dimensions within a slice distance of $r$ using our algorithm 
above. My complexity
analysis is based on the fact that my rendering algorithm can be decomposed
into a pipeline with filtering and drawing steps. I also assume, as
exemplified in~\autoref{sec:gp_applications}, that my points are uniformly
distributed in data space.  My mathematical derivation also assumes that
the ellipsoids generated by the Gaussian process model are, in fact,
hyperballs. While this may seem like an over-simplification, the principal
axes of the ellipsoids are axis-aligned with respect to the parameter 
space (see~\autoref{sec:scene_geometry}).
An ellipse is simply ``stretching'' the parameter space
by a fixed amount in each direction.

The two stages of rendering mean that
the measured time is the time to run the
filtering stage plus the time to run the drawing stage.  However, because
of the pipeline setup of the GPU, a low number of fragments can be drawn
``for free'' on the spare compute capacity of the card not being used for filtering.
Once this spare capacity is exhausted, the rendering time will dominate the
total drawing time.
Therefore,
the total drawing time, $t_\text{total}$, is the time to filter the points
plus the time to render the points on screen but only after a certain 
number of fragments are drawn.  I represent this breaking point with
$I(\text{frags} > a)$ which is an indicator function that returns $0$ when
the number of fragments is less than the break-point, $a$, and $1$ otherwise.

\begin{align}
  t_\text{total} &= t_\text{filter} + I(\text{frags}>a) * t_\text{render}
  \label{eq:acttotal-H}
\end{align}

\subsection{Filtering}
\label{filtering}

In the filtering step (lines \autoref{al:line:disteq} and 
\autoref{al:line:filterif} of \autoref{algo:full}) I take each data
point and compute its distance to each plot in order to determine if it is
worth the effort to actually draw the quad. For each sample point and for each
slice, I compute the distance from the sample point to the slice. If the
distance is less than $r$ I draw it.

Since there are subplots for each pair of dimensions there are
${d \choose 2}$ subplots in total. Filtering is a constant time per point
but is
architecture-dependent. I denote this time $t_f$ and the total 
filtering time, $t_\text{filter}$, is
\begin{align}
  t_\text{filter} &= t_f {d \choose 2} N \text{.}
\end{align}

\subsection{Rendering}
\label{rendering}

During the rendering step, the algorithm only needs to process a fraction of the $N$ points
that are visible. I call this fraction $N'$. 
The rest of the points are discarded in the filtering
code on the GPU. In the case of HyperSlice,
the rendering time is significant. Besides having to determine the size of the
quad to be rendered in lines \autoref{al:line:screenpoint} and 
\autoref{al:line:splatsize} of \autoref{algo:full}, the actual rendering
time depends on the number of pixels covered by the quad since each pixel 
requires a constant time to draw. 
Because of this, my formulation for the 
rendering time 
must include the quad size for each point rendered, $q_i$, and
the time to render each pixel in a quad, $t_\text{H}$,
\begin{align}
  t_\text{render} &= t_\text{H} \sum_{i=1}^{N'} q_i \text{.}
\end{align}

\subsection{Expected total time}
\label{sec:expectedtotaltime}

\autoref{eq:acttotal-H} gives the total running time for a
particular configuration of $N$ sample points in $d$ dimensions and for a particular
viewpoint $\vec{v}$. However, I am interested in how well the rendering
algorithm performs under many different configurations of points and 
viewpoints. 
The worst case performance is when the full kernel needs to be drawn.
In other words, when the kernels are not cut off by the edges
of the parameter space and the view point is in the center.
However, it is important to know how the rendering will
perform as the user views a set of different plots.
Hence, a much more
useful measurement is the \emph{average} time to draw the 
view over all possible point configurations and all
possible viewpoints.
In order to compute this, the expected rendering
time, $E[t_\text{total}]$, is an
average over all point configurations and viewpoints in the unit
cube $[0,1]^d$:

\begin{align}
  E[t_\text{total}] = t_f {d \choose 2} N +
                      I(\text{frags}>a) t_\text{H} E[N']E[q] \text{,}
  \label{eq:exptotal}
\end{align}
Here, the first term represents the time to filter all $N$ points over the
${d \choose 2}$ plots and the second term is the time to draw any points
that pass the filter.

The total amount of rendering that needs to be done is the number of points passing
the filtering stage times the size of these points on the slice.  The quantity
$t_\text{H}$ is the time to draw a single fragment using the HyperSlice
method, $E[N']$ is the expected number of points within a distance of $r$ from
all 2D slices of the subplot matrix, and $E[q]$ is the expected size of a quad
drawn. 

$E[N']$ is based on the total number of sample points we need to process
times the expected percentage of points that will be within range of the slices.
There are $N$ points to process for each of the ${d \choose 2}$ subplots.  
For a single
2D slice, denote the expected percentage of points within a distance $r$ in
$d$ dimensions $\exppts{r}$.  The percentage of points can be expanded into
\begin{align*}
  E[N'] &= {d \choose 2}N \cdot \exppts{r} \\
        &= {d \choose 2}N \cdot \sum_{i=0}^{d-2} (-1)^i {{d-2} \choose i}
                                           \frac{\pi^{d-2-i}r^{d-2+i}}
                                           {\Gamma\left(\frac{d-2+i}{2}+1\right)} \numberthis \label{eq:exppoints} \text{.} \\
\end{align*}
$\exppts{r}$ is the sum of higher and higher dimensional spheres as they
are sliced by $2$-dimensional planes. For the full derivation, please see
\autoref{app:exppts}.

For the HyperSlice technique, the quantity $E[q]$ depends on the size
of the spherical reconstruction kernel which depends on $r$ and $d$. 
Denote this quantity $\expfrags$.  This represents the expected number 
of fragments produced on a particular slice when the sample point is within
a distance $r$ of the slice.  As described in \autoref{app:expfrags} of 
the appendix,
$E[q]$ expands into,
\begin{align*}
  E[q] &= \expfrags \\
       &= \frac{1}{\exppts{r}} 
        \sum_{i=0}^{d-2} (-1)^i {{d-2} \choose i} \left[
          \text{corner}(d,i,r) \right. \\
       & \qquad\qquad\qquad\qquad\qquad
           - \text{side}(d,i,r) \\
       & \qquad\qquad\qquad\qquad\qquad
           + \left. \text{center}(d,i,r) \right] \\
       &= \frac{1}{\exppts{r}} 
        \sum_{i=0}^{d-2} (-1)^i {{d-2} \choose i} \left[
            \frac{4\pi^{(d-i)/2-1} r^{d+i}}{\Gamma((d+i)/2 + 1)} \right. \\
       & \qquad\qquad\qquad\qquad\qquad
           - \frac{3\pi^{(d-i-1)/2} r^{d+i+1}}{\Gamma((d+i+3)/2)} \\
       & \qquad\qquad\qquad\qquad\qquad
           + \left. \frac{2\pi^{(d-i)/2-1} r^{d+i+2}}{\Gamma((d+i)/2+2)} 
        \right]
  \numberthis \label{eq:expfrags} \text{.}
\end{align*}
Here the $\text{corner}(d,i,r)$, $\text{side}(d,i,r)$, and $\text{center}(d,i,r)$ functions correspond to derivations $1$, $2$, and $3$ 
listed in 
\autoref{sec:app:4d_derivation}
of the appendix respectively.
While the current formula appears quite complex, it is fast and easy to 
evaluate on a computer. In fact without this formula the computations would be 
intractable. There might exist a more comprehensible formula,
however, this is beyond the scope of this paper and is a subject for future work.

These formulas take into account the size of the kernel even if part of it is
clipped by the edge of the parameter space.  This is very important for larger
kernels in higher dimensions.  There, the volume of a kernel is very small but
the radius may be very large and therefore the kernel is \emph{always} clipped
by the edges of the parameter space. These corner and edge terms will dominate
in higher dimensional cases or for large values of $r$. In lower dimensional
cases, or smaller values of $r$ the center term will contribute more to the
final rendering time.

