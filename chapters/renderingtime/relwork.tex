\section{Related work}
\label{sec:relatedwork}

The prediction of rendering times is a staple of the 3D rendering community,
(e.g., see early work by Funkhouser and
S{\'e}quin~\cite{Funkhouser:1993}).  However, these are focused on the 
three-dimensional
rendering domain. It has yet to be analyzed in the multi-dimensional domain,
which I do here.  
Another difference with this setting is that I have a known scene geometry 
that I can take into account.
Furthermore, my focus is on the conversion of this high-dimensional data representation
to 2D views and not global illumination.

One could perform an iterative search method, for example bisection search, on
the number of sample points that one could render in interactive time. However,
that would need to be performed for every different combination of
dimensionality, number of samples, and search distance ($d$, $N$, and $r$
respectively). Furthermore, this bisection search may be prohibitively
expensive if we are determining the number of samples of a complex simulation
where each sample takes hours or days to compute. 

\subsection{Multi-D visualization}

Analyzing multi-dimensional data locally is typically done by constraining
each dimension within an interval~\cite{Shneiderman:1994}. Arguably, the
most popular method to visually inspect multi-dimensional data is a scatter
plot or scatter plot matrix (SPloM). Alternatively, one can use
parallel
coordinates~\cite{Inselberg:1985} or some type of radial 
chart~\cite{Jayaraman:2002}.

The
Prosection Matrix~\cite{Tweedie:1998} allows the user to explore the density of
input parameter settings that match certain performance criteria. The user
specifies what constitutes suitable output parameters as well as a ``tolerance
box'' which represents the possible range for input settings. The system then
shows a number of 2D density plots --- one for each pair of parameters ---
indicating how many of the performance criteria were in compliance.
HyperMoVal~\cite{Piringer:2010} also relies on the user to define a slab around
a particular 2D slice. The sample points within this slab are considered
relevant to the view and drawn on screen. The focus of HyperMoVal is
visualizing how well a model fits sampled data. Their desire is to show how
``close'' data points fall to a regression line.

Often, data points represent samples of a continuous function. Hence, it
is quite common to reconstruct this function as best as possible from samples
and visually inspect reconstructed continuous, multi-dimensional function.
In this regard, HyperSlice~\cite{Wijk:1993} plots
two-dimensional orthogonal slices of a continuous function around a local
viewpoint. This allows the user to visually inspect the behavior of the
function around this point. One advantage of HyperSlice is that it improves
the quantitative means for analysis of our multi-dimensional function at 
least locally
by measuring 2D distances. It is difficult to understand distances in
multi-dimensional spaces and 2D has been shown to work better for quantitative
understanding than 3D~\cite{Tory:2006}.

Tuner~\cite{Torsney-Weir:2011} uses the HyperSlice method to
visualize the effects of each parameter around a particular candidate point.
Tuner is focused on finding the optimum parameter settings for a computer
simulation subject to a number of criteria. The optimum parameter setting must
be ``high'' in the sense of maximizing the objective function but also
``stable'' in the sense that changes in the parameter settings will not
produce large changes in the objective measure. This local sensitivity
analysis is visually supported with a HyperSlice view of the high-dimensional
parameter space.

\subsection{Multi-D interpolation}
\label{multi-dinterpolation}

For estimating a continuous function, a popular technique is 
kernel regression~\cite{Simonoff:1996}. In this case, the estimated value at a
particular point in the parameter space, $x'$, is computed by averaging over
all sample points weighted by a kernel function. Formally this can be
expressed as

\begin{align}
 \hat{f}(x) &= \sum_{i=1}^n \varphi(x_i -x') f'(x_i)
 \label{eq:kernel_regression}
\end{align}
for $n$ sample points, $x_i$. The function $\varphi(\cdot)$ is an approximating
kernel. In this case $f'(x_i)$ is the normalized sample value 
of the function $f(\cdot)$ at 
location $x_i \in \real^d$
in parameter space. The normalization factor ensures that we can
compute the known values of the sample points.
This factor is either automatically 
computed as in the case of Gaussian process regression or explicitly computed 
from the local neighborhood. 

Often the squared exponential kernel is chosen for $\varphi(\cdot)$. This 
function has one
or more bandwidth parameters which control the amount of smoothing between 
each sample
point. The amount of smoothing also affects the distance at which a data point
will have an effect on our regression function. While the bandwidth can be
set manually, it can also be set by examining the spatial variation in $f(x)$.
The Gaussian process model (GP)~\cite{Rasmussen:2006}, uses statistical 
variation
to fit the kernel bandwidth appropriately. 
In the squared exponential case,
\begin{align}
  \varphi(x_i - x') &= \sum_{j=1}^d e^{\theta_j (x_{i,j} - x'_j)^2}\text{,}
\end{align}
where $j$ denotes the particular value for a certain dimension so $x_{i,j}$ 
is the value of the $j$th dimension of sample $i$. The value $x_j$ is
the $j$th dimension of the prediction point.
Therefore,
there is a separate parameter, $\theta_j$ to fit for each dimension.
Another approach,
exemplified by Hong et al.~\cite{Hong:2006}, is to set the kernel bandwidth to
take into account the Voronoi cells around each data point. In either case,
I recognize that
setting the bandwidth is data-dependent. Therefore, I test rendering performance for a
number of different kernel sizes.

